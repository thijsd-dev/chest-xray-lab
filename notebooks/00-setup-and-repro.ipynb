{"cells":[{"cell_type":"markdown","source":["# Notebook 0 ‚Äî Config & Reproducible Bootstrap\n","\n","This notebook recreates the assignment setup once, then reuses it everywhere. It downloads the same data, applies the same preprocessing, writes the same train/val/test CSVs, and saves a config.json I carry into Week 1. Goal: a clean, reproducible base so later notebooks don‚Äôt depend on the big original training notebook."],"metadata":{"id":"lKCwVwybrpHX"},"id":"lKCwVwybrpHX"},{"cell_type":"markdown","source":["## Block 1 ‚Äî Environment bootstrap (run twice)\n","\n","This block standardizes the runtime across local + Colab:\n","\n","- Detect Colab, mount Drive, and set PROJECT_ROOT.\n","- Pin exact versions (NumPy first, then PyTorch 2.2.2 CUDA wheels with CPU fallback, then the rest).\n","- Install explainability libs (captum) and CV/ML stack.\n","- Do an editable install of my /src module with --no-deps (reuses helpers from the initial assignment without re-resolving pins).\n","- On Colab, patch requires-python if needed and hard-restart once so ABI/state is clean.\n","- Second run prints versions and exposes safe Drive writers.\n","\n","How to use:\n","- Run once ‚Üí installs & restarts.\n","- Run again ‚Üí verifies, loads\n","- Set CXR_PROJ_ROOT to override the project path locally."],"metadata":{"id":"9M6MDMRg9TYm"},"id":"9M6MDMRg9TYm"},{"cell_type":"code","execution_count":1,"id":"4cae2c6f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4cae2c6f","executionInfo":{"status":"ok","timestamp":1761561686321,"user_tz":-60,"elapsed":6309,"user":{"displayName":"Chossel Dorp","userId":"01873965332713943172"}},"outputId":"2cf920fe-155f-4c1a-ea47-7333930c1524"},"outputs":[{"output_type":"stream","name":"stdout","text":["üêç Python: 3.12.12 | Colab: True\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","‚úÖ Google Drive mounted\n","üìÅ PROJECT ROOT: /content/drive/MyDrive/code/chest-xray-lab\n","python: 3.12.12 | Linux-6.6.105+-x86_64-with-glibc2.35\n","numpy: 1.26.4\n","torch : 2.2.2+cu121 | CUDA? False\n","cv2   : 4.9.0\n","pandas: 2.3.3 | scipy: 1.16.2\n","sklearn: 1.7.2 | skimage: 0.25.2\n","matplotlib: 3.10.7\n"]}],"source":["# --- Block 1 (fixed): Colab-only bootstrap with one-time restart -------------\n","import sys, os, subprocess, platform, time\n","from pathlib import Path\n","\n","# Detect if code is running inside Google Colab by checking loaded modules.\n","# Side effect: enables Colab-specific behavior (Drive mount, pip pins, restart).\n","IN_COLAB = \"google.colab\" in sys.modules\n","print(f\"üêç Python: {sys.version.split()[0]} | Colab: {IN_COLAB}\")\n","\n","def sh(cmd: str):\n","    # Thin wrapper around subprocess.run for shell commands with simple error handling.\n","    # Raises SystemExit on non-zero return to halt the notebook early and surface the failing command.\n","    print(\">>\", cmd)\n","    r = subprocess.run(cmd, shell=True)\n","    if r.returncode != 0:\n","        raise SystemExit(r.returncode)\n","\n","# Marker file used to prevent infinite restarts of the Colab runtime.\n","# Presence of this file indicates the first-run bootstrap has completed.\n","MARK = Path(\"/content/_cxr_bootstrap_done\")\n","\n","# Resolve project root depending on environment.\n","# In Colab: try common Drive locations, else fallback to current working directory.\n","# Outside Colab: allow override via CXR_PROJ_ROOT env var, else cwd.\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\", force_remount=False)  # Avoid remount churn; user can toggle if needed.\n","    print(\"‚úÖ Google Drive mounted\")\n","    CANDIDATES = [\n","        Path(\"/content/drive/MyDrive/code/chest-xray-lab\"),  # Preferred repo path\n","        Path(\"/content/drive/MyDrive/chest_xray_lab\"),       # Alternate naming\n","        Path.cwd(),                                          # Fallback to notebook directory\n","    ]\n","else:\n","    CANDIDATES = [Path(os.environ.get(\"CXR_PROJ_ROOT\", Path.cwd()))]\n","\n","# Pick the first existing candidate as the canonical project root.\n","# Note: if multiple exist, ordering matters; consider logging a warning in multi-match situations.\n","PROJ_ROOT = next((p.resolve() for p in CANDIDATES if p.exists()), Path.cwd().resolve())\n","os.environ[\"CXR_PROJ_ROOT\"] = str(PROJ_ROOT)  # Export for child processes (e.g., pip, editable install).\n","print(\"üìÅ PROJECT ROOT:\", PROJ_ROOT)\n","\n","# Add src/ to the import path to support 'editable-like' imports without installing.\n","SRC_DIR = PROJ_ROOT / \"src\"\n","if str(SRC_DIR) not in sys.path:\n","    sys.path.insert(0, str(SRC_DIR))\n","\n","# First-time bootstrap path (only in Colab and only if MARK doesn't exist yet).\n","# Performs pinning/uninstall to harmonize binary dependencies and avoid ABI mismatches.\n","if IN_COLAB and not MARK.exists():\n","    # Patch 'requires-python' in pyproject for Colab base kernels that lag behind.\n","    # Caution: mutates repository file; safe for Colab but avoid committing this change upstream.\n","    pyproject = PROJ_ROOT / \"pyproject.toml\"\n","    if pyproject.exists():\n","        txt = pyproject.read_text()\n","        if 'requires-python = \">=3.12\"' in txt:\n","            print(\"‚ö†Ô∏è Patching requires-python to >=3.10 for Colab base kernel‚Ä¶\")\n","            pyproject.write_text(txt.replace('requires-python = \">=3.12\"', 'requires-python = \">=3.10\"'))\n","\n","    # Clean slate to prevent silent ABI conflicts (NumPy/Torch/Scipy stack).\n","    # Set CXR_FORCE_CLEAN=0 to skip aggressive uninstalls when debugging.\n","    if os.environ.get(\"CXR_FORCE_CLEAN\", \"1\") == \"1\":\n","        sh(\"pip -q uninstall -y \"\n","           \"torch torchvision torchaudio \"\n","           \"numpy pandas scipy scikit-learn scikit-image \"\n","           \"matplotlib \"\n","           \"opencv-python opencv-contrib-python opencv-python-headless \"\n","           \"jax jaxlib pillow tabulate kagglehub captum torchcam || true\")\n","        sh(\"pip -q install --upgrade pip\")  # Keep pip recent to reduce resolver quirks.\n","\n","    # 1) Pin NumPy FIRST to a version compatible with PyTorch 2.2 wheels on Colab.\n","    # Rationale: prevents resolver from upgrading NumPy to an ABI that mismatches Torch.\n","    sh(\"pip install --no-cache-dir numpy==1.26.4\")\n","\n","    # 2) Install Torch 2.2.2 with CUDA wheels where available; fallback logic handles Colab GPU variants.\n","    # Tries CUDA 12.1 then 11.8. If both fail (e.g., CPU runtime), installs CPU wheels from Torch index.\n","    for wheels in (\n","        \"torch==2.2.2+cu121 torchvision==0.17.2+cu121 torchaudio==2.2.2+cu121\",\n","        \"torch==2.2.2+cu118 torchvision==0.17.2+cu118 torchaudio==2.2.2+cu118\",\n","    ):\n","        try:\n","            sh(f\"pip install --no-cache-dir -f https://download.pytorch.org/whl/torch_stable.html {wheels}\")\n","            break\n","        except SystemExit:\n","            pass\n","    else:\n","        print(\"‚ö†Ô∏è GPU wheels failed; installing CPU wheels\")\n","        sh(\"pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cpu \"\n","           \"torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2\")\n","\n","    # 3) Install the rest of the scientific stack with strict pins where stability is critical.\n","    # Matplotlib pinned with fallback spec to accommodate transient wheel availability on Colab images.\n","    try:\n","        sh(\"pip install --no-cache-dir matplotlib==3.10.7\")\n","    except SystemExit:\n","        sh(\"pip install --no-cache-dir 'matplotlib>=3.10,<3.11'\")\n","    sh(\"pip install --no-cache-dir pandas==2.3.3 scipy==1.16.2 scikit-learn==1.7.2 scikit-image==0.25.2 tabulate==0.9.0 kagglehub==0.3.13\")\n","    try:\n","        sh(\"pip install --no-cache-dir opencv-python==4.9.0.80\")\n","    except SystemExit:\n","        # Headless fallback avoids GUI backends not present on Colab; functionality is similar for CV workloads.\n","        sh(\"pip install --no-cache-dir opencv-python-headless==4.9.0.80\")\n","\n","    # 4) Model interpretability tooling (optional extras).\n","    # Captum is aligned with Torch 2.2; TorchCAM kept optional to encourage custom Grad-CAM if desired.\n","    sh(\"pip install --no-cache-dir captum==0.7.0\")\n","    # Optional: drop torchcam and implement Grad-CAM yourself\n","    # sh(\"pip install --no-cache-dir torchcam==0.4.0\")\n","\n","    # 5) Editable install of the repo WITHOUT dependency resolution.\n","    # --no-deps ensures previously pinned wheels aren't re-resolved by project metadata.\n","    sh(f\"pip install --no-cache-dir -e {PROJ_ROOT} --no-deps\")\n","\n","    # 6) Create marker and force a hard restart to ensure the runtime imports the freshly pinned ABIs.\n","    # Using SIGKILL avoids partial state; the guard MARK prevents infinite restart loops.\n","    MARK.touch()\n","    import os as _os\n","    _os.kill(_os.getpid(), 9)\n","\n","# Second run (post-restart): import and print versions to verify environment health.\n","# Skips all uninstall/install work; functions as a quick sanity check plus utility defs.\n","if IN_COLAB:\n","    import numpy as np, torch, cv2, pandas, scipy, sklearn, skimage, matplotlib\n","    print(\"python:\", sys.version.split()[0], \"|\", platform.platform())\n","    print(\"numpy:\", np.__version__)\n","    print(\"torch :\", torch.__version__, \"| CUDA?\", torch.cuda.is_available())  # Note: torch.cuda.is_available() reflects driver/runtime availability.\n","    print(\"cv2   :\", cv2.__version__)\n","    print(\"pandas:\", pandas.__version__, \"| scipy:\", scipy.__version__)\n","    print(\"sklearn:\", sklearn.__version__, \"| skimage:\", skimage.__version__)\n","    print(\"matplotlib:\", matplotlib.__version__)\n","\n","    # Lightweight, robust file writers for Google Drive to mitigate sync lag / buffering issues.\n","    # safe_write_bytes ensures data durability by fsync + size check with retries.\n","    import io\n","    def safe_write_bytes(path: Path, data: bytes, retries: int = 3, sleep_s: float = 0.5):\n","        path.parent.mkdir(parents=True, exist_ok=True)\n","        with open(path, \"wb\") as f:\n","            f.write(data); f.flush(); os.fsync(f.fileno())\n","        for _ in range(retries):\n","            if path.exists() and path.stat().st_size == len(data):\n","                return True\n","            time.sleep(sleep_s)\n","        raise IOError(f\"Drive sync failed for {path}\")\n","    def safe_write_text(path: Path, text: str): return safe_write_bytes(path, text.encode(\"utf-8\"))\n","    globals().update(dict(safe_write_bytes=safe_write_bytes, safe_write_text=safe_write_text))\n","    # Note: safe_write_* return True on success; callers may want to assert the return or handle exceptions.\n"]},{"cell_type":"markdown","source":["## Block 2 ‚Äî Paths, env, and package import\n","\n","This block wires up paths and avoids duplication:\n","\n","- Reads `CXR_PROJ_ROOT` from Block 1 and sets env vars for raw/processed data, manifests, and a fixed seed.\n","- Installs `chest_xray_lab` in **editable mode** only if it‚Äôs not already importable (prevents double-installs between local and Colab).\n","- Imports the canonical config (`PROJ_ROOT`, `RAW_DIR`, `PROC_DIR`, `MANIFESTS`, `DEVICE`, `SEED`) so every notebook uses the same ‚Äúsingle source of truth‚Äù.\n","- Ensures the expected directories exist and prints a quick device summary (CUDA/MPS/CPU).\n"],"metadata":{"id":"9lWPaiPyspRj"},"id":"9lWPaiPyspRj"},{"cell_type":"code","execution_count":2,"id":"3e2e7eaa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3e2e7eaa","executionInfo":{"status":"ok","timestamp":1761561687120,"user_tz":-60,"elapsed":781,"user":{"displayName":"Chossel Dorp","userId":"01873965332713943172"}},"outputId":"cb55963e-0d12-4c28-9616-a4aeaefa591b"},"outputs":[{"output_type":"stream","name":"stdout","text":["üìÅ PROJ_ROOT: /content/drive/MyDrive/code/chest-xray-lab\n","üìÅ RAW_DIR  : /content/drive/MyDrive/code/chest-xray-lab/data/raw/chest_xray\n","üìÅ PROC_DIR : /content/drive/MyDrive/code/chest-xray-lab/data/processed/chest_xray_split\n","üìÅ MANIFESTS: /content/drive/MyDrive/code/chest-xray-lab/data/processed/manifests\n","‚ö™Ô∏è CPU-only\n"]}],"source":["# --- Block 2: paths/env + editable install (no duplication) ------------------\n","import os, importlib.util, subprocess, pathlib\n","from pathlib import Path\n","\n","# 1) Ensure env vars ONCE (Block 1 already set CXR_PROJ_ROOT)\n","# Read the project root from the environment and normalize to an absolute path.\n","# setdefault(...) writes defaults only if the variable is not already defined,\n","# which lets advanced users override via environment without editing code.\n","PR = Path(os.environ[\"CXR_PROJ_ROOT\"]).resolve()\n","os.environ.setdefault(\"CXR_RAW_DIR\",   str(PR / \"data\" / \"raw\" / \"chest_xray\"))          # Source dataset location (immutable copies ideally).\n","os.environ.setdefault(\"CXR_PROC_DIR\",  str(PR / \"data\" / \"processed\" / \"chest_xray_split\")) # Derived/processed splits live here (safe to regenerate).\n","os.environ.setdefault(\"CXR_MANIFESTS\", str(PR / \"data\" / \"processed\" / \"manifests\"))     # CSV/JSON manifests for reproducible pipelines.\n","os.environ.setdefault(\"CXR_SEED\", \"42\")                                                   # Global seed as string (convert to int at use-sites).\n","\n","# 2) Editable install only if package not importable\n","# If the project package is not importable, install in editable mode.\n","# This avoids duplicate installs and ensures local code edits are immediately reflected.\n","# Pitfall: mixing 'chest-xray-lab' (dist-name) and 'chest_xray_lab' (import-name) is intentional;\n","# uninstall targets the distribution, import checks the module. Keep both consistent with pyproject.\n","if importlib.util.find_spec(\"chest_xray_lab\") is None:\n","    print(\"‚Ñπ Installing 'chest_xray_lab' in editable mode...\")\n","    subprocess.run([\"python\", \"-m\", \"pip\", \"uninstall\", \"-y\", \"chest-xray-lab\"], check=False)  # Best-effort cleanup; ignore failure.\n","    subprocess.check_call([\"python\", \"-m\", \"pip\", \"install\", \"-e\", str(PR)])                   # Editable install binds imports to local source tree.\n","\n","# 3) Import from config (single source of truth) and ensure dirs\n","# Centralized config ensures paths/devices/seeds are defined once and reused everywhere.\n","# Importing here asserts the package is importable and that config resolves env vars to Path objects.\n","from chest_xray_lab.config import PROJ_ROOT, RAW_DIR, PROC_DIR, MANIFESTS, DEVICE, SEED\n","\n","# Create required directories idempotently. parents=True creates nested paths; exist_ok=True avoids errors if already present.\n","for d in (RAW_DIR, PROC_DIR, MANIFESTS):\n","    d.mkdir(parents=True, exist_ok=True)\n","\n","# Log resolved paths for transparency and easier debugging in shared notebooks.\n","print(\"üìÅ PROJ_ROOT:\", PROJ_ROOT)\n","print(\"üìÅ RAW_DIR  :\", RAW_DIR)\n","print(\"üìÅ PROC_DIR :\", PROC_DIR)\n","print(\"üìÅ MANIFESTS:\", MANIFESTS)\n","\n","# 4) Device summary\n","# Provide a concise runtime device report, preferring Apple MPS (Metal) when available,\n","# then CUDA, else CPU. This is purely informational‚Äîmodel code should still query DEVICE from config.\n","import torch\n","if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n","    print(\"üü¢ Apple MPS available\")\n","elif torch.cuda.is_available():\n","    print(f\"üü¢ CUDA device: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"‚ö™Ô∏è CPU-only\")\n"]},{"cell_type":"markdown","source":["## Block 3 ‚Äî Core imports, seed, and runtime summary\n","\n","This block pulls in the core libs, locks the global seed, and prints a quick runtime report.\n","\n","- Uses `set_global_seed(SEED)` from my utils so runs are deterministic by default.\n","- `FAST_GPU=False` keeps cudnn in deterministic mode; flip to `True` only if you accept small nondeterminism for speed.\n","- Sets `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` to reduce CUDA memory fragmentation (no-op on CPU/MPS).\n","- Prints Python/NumPy/OpenCV/Matplotlib/Torch versions and whether CUDA is available.\n"],"metadata":{"id":"pC8kBD5tsyMa"},"id":"pC8kBD5tsyMa"},{"cell_type":"code","execution_count":3,"id":"863324fc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"863324fc","executionInfo":{"status":"ok","timestamp":1761561687710,"user_tz":-60,"elapsed":573,"user":{"displayName":"Chossel Dorp","userId":"01873965332713943172"}},"outputId":"eac339e9-52db-4733-9e57-f474fd2099cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["üî¢ Seed: 42\n","Python: 3.12.12\n","NumPy : 1.26.4 | OpenCV: 4.9.0 | Matplotlib: 3.10.7\n","Torch : 2.2.2+cu121\n","CUDA available: False\n"]}],"source":["# --- Core imports ------------------------------------------------------------\n","import os, sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2, torch, torch.nn as nn\n","\n","from chest_xray_lab.config import SEED\n","from chest_xray_lab.utils.repro import set_global_seed\n","\n","# --- Reproducibility (deterministic by default) ------------------------------\n","FAST_GPU = False   # set True only if you accept minor nondeterminism for speed\n","set_global_seed(SEED, deterministic=not FAST_GPU, fast_gpu=FAST_GPU)\n","\n","\n","# optional (no-op on CPU/MPS)\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","# --- Versions ---------------------------------------------------------------\n","import matplotlib\n","print(f\"üî¢ Seed: {SEED}\")\n","print(f\"Python: {sys.version.split()[0]}\")\n","print(f\"NumPy : {np.__version__} | OpenCV: {cv2.__version__} | Matplotlib: {matplotlib.__version__}\")\n","print(f\"Torch : {torch.__version__}\")\n","print(\"CUDA available:\", torch.cuda.is_available())\n"]},{"cell_type":"markdown","id":"52778039","metadata":{"id":"52778039"},"source":["## Block 4 ‚Äî Download raw data (idempotent)\n","\n","This block downloads the Chest X-Ray Pneumonia dataset via `kagglehub` and lays it out under `CXR_RAW_DIR` with the expected `train/val/test` structure.\n","\n","- Targets the same dataset as the assignment (`paultimothymooney/chest-xray-pneumonia`).\n","- Finds the **shallowest** directory that already contains `train/val/test` (handles extra nesting in Kaggle zips).\n","- Copies into `CXR_RAW_DIR` if missing; otherwise **skips** existing non-empty splits (safe to re-run).\n","- Prints a tiny count summary per split so we can sanity-check the download.\n","\n","Inputs: `CXR_RAW_DIR` from env (set earlier).  \n","Outputs: populated `train/val/test` folders under `CXR_RAW_DIR`, plus a human-readable summary.\n","\n","> Re-running is a no-op unless the target is empty.\n"]},{"cell_type":"code","execution_count":4,"id":"b17cffbf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b17cffbf","executionInfo":{"status":"ok","timestamp":1761561854314,"user_tz":-60,"elapsed":166601,"user":{"displayName":"Chossel Dorp","userId":"01873965332713943172"}},"outputId":"74dd22a0-b1b6-4d50-eb70-089d106dffd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Colab cache for faster access to the 'chest-xray-pneumonia' dataset.\n","Downloaded to cache: /kaggle/input/chest-xray-pneumonia\n","Using split root: /kaggle/input/chest-xray-pneumonia/chest_xray\n","Copying train: /kaggle/input/chest-xray-pneumonia/chest_xray/train -> /content/drive/MyDrive/code/chest-xray-lab/data/raw/chest_xray/train\n","Copying val: /kaggle/input/chest-xray-pneumonia/chest_xray/val -> /content/drive/MyDrive/code/chest-xray-lab/data/raw/chest_xray/val\n","Copying test: /kaggle/input/chest-xray-pneumonia/chest_xray/test -> /content/drive/MyDrive/code/chest-xray-lab/data/raw/chest_xray/test\n","üì¶ Prepared at: /content/drive/MyDrive/code/chest-xray-lab/data/raw/chest_xray\n","üìä Counts: {'train': 5216, 'val': 16, 'test': 624}\n"]}],"source":["# make_dataset_raw_min.py\n","from pathlib import Path\n","import shutil\n","import kagglehub\n","import os\n","\n","# Resolve the raw dataset directory from env and ensure it exists.\n","RAW_DIR = Path(os.environ[\"CXR_RAW_DIR\"]).resolve()\n","RAW_DIR.mkdir(parents=True, exist_ok=True)\n","\n","def _has_split(root: Path) -> bool:\n","    # Check if non-empty train/val/test subdirs exist (any file anywhere inside).\n","    # Note: 'any(glob(\"**/*\"))' treats empty dirs as \"absent\".\n","    return all((root / s).is_dir() and any((root / s).glob(\"**/*\"))\n","               for s in (\"train\", \"val\", \"test\"))\n","\n","def _find_split_root(cache_root: Path) -> Path:\n","    # Walk cache_root and find the SHALLOWEST dir that directly contains\n","    # train/val/test (case-insensitive via .name.lower()).\n","    # Skips paths within __MACOSX bundles that can appear in zips.\n","    candidates = []\n","    for p in cache_root.rglob(\"*\"):\n","        if not p.is_dir():\n","            continue\n","        names = {c.name.lower() for c in p.iterdir() if c.is_dir()}\n","        if {\"train\", \"val\", \"test\"}.issubset(names):\n","            if \"__macosx\" in {part.lower() for part in p.parts}:\n","                continue\n","            candidates.append((len(p.parts), p))\n","    if not candidates:\n","        # Defensive: dataset structure changed or missing; fail with context.\n","        raise FileNotFoundError(f\"Could not locate train/val/test under {cache_root}\")\n","    # Return shallowest candidate to avoid nested duplicates.\n","    return sorted(candidates, key=lambda t: t[0])[0][1]\n","\n","def _copy_split(src_root: Path, dst_root: Path):\n","    # Copy each split once; if destination exists and is non-empty, skip.\n","    # dirs_exist_ok=True lets copytree merge into existing dirs (Py3.8+).\n","    for split in (\"train\", \"val\", \"test\"):\n","        src = (src_root / split).resolve()\n","        dst = (dst_root / split).resolve()\n","        if dst.exists() and any(dst.glob(\"**/*\")):\n","            print(f\"‚Ü™ Skip existing '{split}' at {dst}\")\n","            continue\n","        print(f\"Copying {split}: {src} -> {dst}\")\n","        shutil.copytree(src, dst, dirs_exist_ok=True)\n","\n","def _count_files(folder: Path) -> int:\n","    # Count all regular files under a folder (recursive).\n","    return sum(1 for f in folder.rglob(\"*\") if f.is_file())\n","\n","def prepare_raw_dataset():\n","    # Idempotent entrypoint: if dataset already present, do nothing.\n","    if _has_split(RAW_DIR):\n","        print(\"‚úÖ Raw dataset already present at:\", RAW_DIR)\n","    else:\n","        # Pull dataset to a local cache via kagglehub; returns the cache path.\n","        cache_root = Path(kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")).resolve()\n","        print(\"Downloaded to cache:\", cache_root)\n","        # Detect the folder that holds train/val/test and copy into RAW_DIR.\n","        split_root = _find_split_root(cache_root)\n","        print(\"Using split root:\", split_root)\n","        _copy_split(split_root, RAW_DIR)\n","\n","    # Lightweight human-readable counts (no manifest creation here).\n","    summary = {s: _count_files(RAW_DIR / s) for s in (\"train\", \"val\", \"test\")}\n","    print(\"üì¶ Prepared at:\", RAW_DIR)\n","    print(\"üìä Counts:\", summary)\n","\n","if __name__ == \"__main__\":\n","    # Allow running as a script (e.g., `python make_dataset_raw_min.py`).\n","    prepare_raw_dataset()\n"]},{"cell_type":"markdown","source":["## Block 5 ‚Äî Patient-wise split only\n","\n","This cell performs the patient-wise split identical to how it was done in the initial assignment.\n","- Groups by patient to prevent leakage.\n","- Stratifies into `train/val/test` with `test_frac=0.11`, `val_frac=0.09`, `seed=42`.\n","- Returns `train_items`, `val_items`, `test_items`, `POS_WEIGHT`."],"metadata":{"id":"Yc5XYIRjtWVL"},"id":"Yc5XYIRjtWVL"},{"cell_type":"code","execution_count":5,"id":"dee296c7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dee296c7","executionInfo":{"status":"ok","timestamp":1761561870414,"user_tz":-60,"elapsed":16095,"user":{"displayName":"Chossel Dorp","userId":"01873965332713943172"}},"outputId":"e102395b-b289-4698-cb79-72a4d648ec18"},"outputs":[{"output_type":"stream","name":"stdout","text":["== Patient-level split summary ==\n","split   total   NORMAL   PNEUM.    Pos%   patients\n","train    4719     1232     3487   73.9%       2538\n","val       530      141      389   73.4%        286\n","test      607      210      397   65.4%        350\n","\n","Patient overlap (should be 0): train‚à©val=0, train‚à©test=0, val‚à©test=0\n"]}],"source":["from pathlib import Path\n","from chest_xray_lab.utils.split import split_by_patient\n","\n","# Resolve raw dataset dir from env (must be set by earlier blocks/config).\n","RAW_DIR = Path(os.environ[\"CXR_RAW_DIR\"])  # e.g., $CXR_PROJ_ROOT/data/raw/chest_xray\n","\n","# Patient-wise split to prevent leakage (images from same patient never cross splits).\n","# seed controls reproducibility; test/val fractions are proportions of the whole set.\n","splits = split_by_patient(RAW_DIR, seed=42, test_frac=0.11, val_frac=0.09)\n","\n","# Unpack convenience views.\n","# Each item: (absolute_image_path: Path, label: str/int, patient_id: str)\n","train_items = splits[\"train\"]\n","val_items   = splits[\"val\"]\n","test_items  = splits[\"test\"]\n","\n","# Class imbalance weight for positive class; typically used in BCEWithLogitsLoss(pos_weight=...).\n","POS_WEIGHT  = splits[\"pos_weight\"]\n"]},{"cell_type":"markdown","source":["## Block 6 ‚Äî Reproduce preprocessing & warm the cache (assignment-parity)\n","\n","This cell **recreates the exact preprocessing pipeline** used in the initial assignment for both training and evaluation, then warms a PNG cache under `CXR_PROC_DIR`.\n","\n","- Uses a versioned `PreprocConfig` (`cover_crop ‚Üí resize ‚Üí pad ‚Üí 224√ó224`) to match the original pipeline.\n","- Caches results **idempotently** (write only if missing), so later notebooks read the same tensors the model saw.\n","- These cached images are later written to file/Drive and consumed by the Week-1 tasks to:\n","  - preserve the **same train/val/test splits** the `best_model.pt` was trained/tested on,\n","  - reproduce the **same dataset mean/std** assumptions used in Week-1 (by drawing from the same preprocessed set).\n","\n","Outcome: a deterministic, reusable cache that mirrors the original training/eval data exactly.\n"],"metadata":{"id":"KNI2zm9ftlXg"},"id":"KNI2zm9ftlXg"},{"cell_type":"code","execution_count":6,"id":"4fb99e8d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fb99e8d","executionInfo":{"status":"ok","timestamp":1761562231998,"user_tz":-60,"elapsed":361582,"user":{"displayName":"Chossel Dorp","userId":"01873965332713943172"}},"outputId":"aee611ad-b4d0-4931-deca-c984908690b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["preproc hash: 3a0b791144\n","warmed 4719 items | new files written: 4719\n","warmed 530 items | new files written: 530\n","warmed 607 items | new files written: 607\n"]}],"source":["from pathlib import Path\n","from chest_xray_lab.utils.preproc import PreprocConfig, preproc_hash, cache_path, ensure_cached_png\n","import cv2, numpy as np\n","\n","# Resolve dataset roots from environment. (Ensure earlier blocks set these.)\n","RAW_DIR  = Path(os.environ[\"CXR_RAW_DIR\"])\n","PROC_DIR = Path(os.environ[\"CXR_PROC_DIR\"])\n","\n","# Preprocessing configuration (single source of truth).\n","# target_hw: final H√óW; cover_crop keeps aspect by center-cropping after scaling.\n","# pad_mode: how borders are padded if needed; \"reflect\" avoids hard edges.\n","# dark_frac/min_keep_run: heuristics for removing large dark margins/borders.\n","CFG = PreprocConfig(\n","    target_hw=(224,224),\n","    resize_mode=\"cover_crop\",   # matches your old _RESIZE_MODE\n","    pad_mode=\"reflect\",\n","    dark_frac=0.80,\n","    min_keep_run=8,\n",")\n","\n","# Stable hash of CFG for cache versioning; changes invalidate/segregate caches.\n","PHASH = preproc_hash(CFG)\n","print(\"preproc hash:\", PHASH)\n","\n","def warm_cache(items):  # items = list of (abs_path, label, patient_id)\n","    # Materialize preprocessed PNGs for a split to avoid on-the-fly work later.\n","    new = 0\n","    for abs_path, *_ in items:\n","        # Compute path of the cached PNG relative to RAW_DIR + CFG.\n","        rel = str(Path(abs_path).resolve().relative_to(RAW_DIR))\n","        dst = cache_path(PROC_DIR, CFG, rel)\n","        if not dst.exists():\n","            # Performs crop + cover-resize + write PNG atomically (via library util).\n","            ensure_cached_png(Path(abs_path), dst, CFG)  # does crop + cover-resize + write PNG\n","            new += 1\n","    print(f\"warmed {len(items)} items | new files written: {new}\")\n","\n","# Populate cache for all splits; idempotent (skips files that already exist).\n","warm_cache(train_items)\n","warm_cache(val_items)\n","warm_cache(test_items)\n"]},{"cell_type":"markdown","id":"e12f0534","metadata":{"id":"e12f0534"},"source":["## Block 7 ‚Äî Load checkpoint, bind cached data, and sanity-check\n","\n","This cell rebuilds the exact model, loads the saved `best_model.pt`, and points it at the cached PNGs from Block 6.\n","\n","- Recreates split ‚Üí cache pairs (`to_cached_pairs`) using the same `CFG` and `cache_path`.\n","- Runs a quick eval on `test` to sanity-check that this setup matches the original assignment (tiny drift possible from package/runtime versions, but split + preprocessing are identical).\n","- Computes the train-set mean/std in `[0,1]` to serve as the occlusion baseline for MoRF/SOFI in Week-1.\n","\n","**Observed (this run):** `TEST: AUC=0.996, Acc=0.965, F1=0.973`  \n","**Original assignment:** `TEST: AUC=0.996, Acc=0.962, F1=0.971`  \n","**Train mean:** `0.575170` now vs `0.575157` before (negligible).\n","\n","Conclusion: metrics are effectively identical; the cache + checkpoint wiring is faithful to the original setup."]},{"cell_type":"code","execution_count":7,"id":"5ee4077a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ee4077a","executionInfo":{"status":"ok","timestamp":1761562370096,"user_tz":-60,"elapsed":138102,"user":{"displayName":"Chossel Dorp","userId":"01873965332713943172"}},"outputId":"e06279fd-b948-4cca-c836-c9ae358495da"},"outputs":[{"output_type":"stream","name":"stdout","text":["TEST:  AUC=0.996  Acc=0.965  F1=0.973\n","SHUF AUC (labels permuted, predictions fixed): 0.527\n","[Occlusion] train mean in [0,1]: 0.575170 (std=0.172597)\n"]}],"source":["from chest_xray_lab.config import DEVICE, CKPT_PATH, RAW_DIR, PROC_DIR, SEED\n","from chest_xray_lab.models.build import build_model\n","from chest_xray_lab.models.load  import load_checkpoint\n","from chest_xray_lab.data.cache_io import to_cached_pairs, mean_std_from_cached_pairs\n","from chest_xray_lab.data.dataset_eval import make_eval_loader\n","from chest_xray_lab.eval.metrics import collect_logits, compute_epoch_metrics, compute_shuf_auc\n","\n","# Build + load\n","model = build_model(\"efficientnet_b0\", pretrained=False, in_chans=1).to(DEVICE)\n","model = load_checkpoint(model, CKPT_PATH, map_location=DEVICE)\n","\n","# Map split items -> cached PNG pairs using the preprocessing CFG and cache_path from earlier.\n","# Each element becomes (cached_png_path, label). Reuses disk cache; does not recompute if present.\n","cached_train = to_cached_pairs(train_items, RAW_DIR, PROC_DIR, CFG, cache_path)\n","cached_val   = to_cached_pairs(val_items,   RAW_DIR, PROC_DIR, CFG, cache_path)\n","cached_test  = to_cached_pairs(test_items,  RAW_DIR, PROC_DIR, CFG, cache_path)\n","\n","# (Optional) quick sanity eval\n","# Eval loader: deterministic (no shuffle), appropriate transforms for cached PNGs.\n","test_dl = make_eval_loader(cached_test, batch_size=32)\n","# Collect logits for the whole split; y_logits are raw scores (sigmoid applied inside metrics if expected).\n","y_true, y_logits = collect_logits(model, test_dl, DEVICE)\n","# Threshold-dependent metrics (Acc/F1) at 0.5; AUROC is threshold-free.\n","metrics = compute_epoch_metrics(y_true, y_logits, thresh=0.5)\n","# Label permutation baseline: keeps predictions fixed, shuffles labels (expected AUROC ‚âà 0.5 if sane).\n","shuf_auc = compute_shuf_auc(y_true, y_logits, seed=SEED)\n","print(f\"TEST:  AUC={metrics['auroc']:.3f}  Acc={metrics['acc']:.3f}  F1={metrics['f1']:.3f}\")\n","print(f\"SHUF AUC (labels permuted, predictions fixed): {shuf_auc:.3f}\")\n","\n","# Baseline stats for occlusion-style evals (e.g., MoRF/insertion): mean/std in [0,1] space from training set.\n","mean01, std01 = mean_std_from_cached_pairs(cached_train)\n","print(f\\\"[Occlusion] train mean in [0,1]: {mean01:.6f} (std={std01:.6f})\\\")\n"]},{"cell_type":"markdown","id":"ba912468","metadata":{"id":"ba912468"},"source":["## Block 8 ‚Äî Persist manifests & minimal config for Week-1\n","\n","This cell writes everything Week-1 needs to disk:\n","\n","- CSVs with cached `(png_path, label)` for `train/val/test`.\n","- A minimal `part0_min.json` with checkpoint path, seed, preprocessing hash, and the `[0,1]` train mean/std used for occlusion baselines.\n","- All paths are absolute so downstream notebooks can load without extra setup.\n","\n","**Colab note:** Google Drive can lag. Newly written files may take a little time to appear in Drive. Give it a short pause before opening the Week-1 notebook to avoid ‚Äúfile not found‚Äù hiccups.\n","\n","Outputs:\n","- `${MANIFESTS}/train_cached.csv`, `val_cached.csv`, `test_cached.csv`\n","- `${MANIFESTS}/part0_min.json`\n","\n","These are the artifacts Week-1 reads to reproduce the initial assignment's setup."]},{"cell_type":"code","execution_count":8,"id":"16434813","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16434813","executionInfo":{"status":"ok","timestamp":1761562370110,"user_tz":-60,"elapsed":11,"user":{"displayName":"Chossel Dorp","userId":"01873965332713943172"}},"outputId":"8f08452d-0a1f-4db7-c873-fde713f8b369"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote CSVs: /content/drive/MyDrive/code/chest-xray-lab/data/processed/manifests/train_cached.csv /content/drive/MyDrive/code/chest-xray-lab/data/processed/manifests/val_cached.csv /content/drive/MyDrive/code/chest-xray-lab/data/processed/manifests/test_cached.csv\n","Wrote cfg : /content/drive/MyDrive/code/chest-xray-lab/data/processed/manifests/part0_min.json\n"]}],"source":["# --- Minimal manifests for Week-1 (no meta folder) --------------------------\n","import csv, json, hashlib\n","from pathlib import Path\n","\n","from chest_xray_lab.config import PROJ_ROOT, MANIFESTS as MANI_DIR, CKPT_PATH, SEED\n","# If you didn't compute mean01,std01 earlier, import helper:\n","# from chest_xray_lab.data.cache_io import mean_std_from_cached_pairs\n","\n","# Ensure manifests directory exists (idempotent).\n","MANI_DIR.mkdir(parents=True, exist_ok=True)\n","\n","def _write_pairs_csv(pairs, out_csv: Path):  # pairs: [(png_path,label), ...]\n","    # Serialize cached pairs to a simple 2-column CSV for downstream loaders.\n","    out_csv.parent.mkdir(parents=True, exist_ok=True)\n","    with open(out_csv, \"w\", newline=\"\") as f:\n","        w = csv.writer(f); w.writerow([\"png_path\",\"label\"])\n","        for p, y in pairs:\n","            w.writerow([p, int(y)])  # force int for consistent parsing\n","\n","def _filelist_md5(pairs):\n","    # Stable checksum of the file list (sorted) for quick change detection.\n","    items = \"\\n\".join(sorted([p for p,_ in pairs])).encode()\n","    return hashlib.md5(items).hexdigest()\n","\n","# 1) Split CSVs\n","# Export per-split lists of (cached_png_path, label).\n","train_csv = MANI_DIR / \"train_cached.csv\"\n","val_csv   = MANI_DIR / \"val_cached.csv\"\n","test_csv  = MANI_DIR / \"test_cached.csv\"\n","_write_pairs_csv(cached_train, train_csv)\n","_write_pairs_csv(cached_val,   val_csv)\n","_write_pairs_csv(cached_test,  test_csv)\n","\n","# 2) Baseline stats in [0,1]\n","# Prefer previously computed mean/std (avoids recompute and keeps runs aligned).\n","train_mean01, train_std01 = float(mean01), float(std01)\n","# If you didn't compute them yet in this session, uncomment:\n","# train_mean01, train_std01 = mean_std_from_cached_pairs(cached_train)\n","\n","# 3) Minimal config JSON (absolute paths; week-1 notebook can read only this)\n","# Acts as a single source of truth for evaluation/setup.\n","part0_min = {\n","    \"ckpt_path\": str(CKPT_PATH),\n","    \"seed\": int(SEED),\n","    \"preproc_hash\": PHASH,                     # from your preproc block\n","    \"train_mean01\": train_mean01,\n","    \"train_std01\":  train_std01,\n","    \"csv\": {\n","        \"train\": str(train_csv),\n","        \"val\":   str(val_csv),\n","        \"test\":  str(test_csv),\n","    },\n","    \"manifests_dir\": str(MANI_DIR)\n","}\n","part0_min_path = MANI_DIR / \"part0_min.json\"\n","part0_min_path.write_text(json.dumps(part0_min, indent=2))\n","\n","print(\"Wrote CSVs:\", train_csv, val_csv, test_csv)\n","print(\"Wrote cfg :\", part0_min_path)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}